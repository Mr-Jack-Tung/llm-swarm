# llm-swarm

This repo is intended for generating massive text leverage [huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference) and [vLLM](https://github.com/vllm-project/vllm)

Prerequisites:
* A slurm cluster
* docker


## Install and prepare

```bash
pip install -e .
# or pip install -r ./examples/hh/requirements.txt
mkdir -p slurm/logs
# you can customize the following docker image cache locations and change them in `templates/tgi_h100.template.slurm` and `templates/vllm_h100.template.slurm`
mkdir -p .cache/
```

## Hello world

```bash
export HF_TOKEN=<YOUR_HF_TOKEN>
python examples/hello_world.py
python examples/hello_world_vllm.py
```

```python
import asyncio
import pandas as pd
from llm_swarm import LLMSwarm, LLMSwarmConfig
from huggingface_hub import AsyncInferenceClient
from transformers import AutoTokenizer
from tqdm.asyncio import tqdm_asyncio


tasks = [
    "What is the capital of France?",
    "Who wrote Romeo and Juliet?",
    "What is the formula for water?"
]
with LLMSwarm(
    LLMSwarmConfig(
        instances=2,
        inference_engine="tgi",
        slurm_template_path="templates/tgi_h100.template.slurm",
        load_balancer_template_path="templates/nginx.template.conf",
    )
) as llm_swarm:
    client = AsyncInferenceClient(model=llm_swarm.endpoint)
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")
    tokenizer.add_special_tokens({"sep_token": "", "cls_token": "", "mask_token": "", "pad_token": "[PAD]"})

    async def process_text(task):
        prompt = tokenizer.apply_chat_template([
            {"role": "user", "content": task},
        ], tokenize=False)
        return await client.text_generation(
            prompt=prompt,
            max_new_tokens=200,
        )

    async def main():
        results = await tqdm_asyncio.gather(*(process_text(task) for task in tasks))
        df = pd.DataFrame({'Task': tasks, 'Completion': results})
        print(df)
    asyncio.run(main())
```
```
(.venv) costa@login-node-1:/fsx/costa/llm-swarm$ python examples/hello_world.py
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
running sbatch --parsable slurm/tgi_1705591874_tgi.slurm
running sbatch --parsable slurm/tgi_1705591874_tgi.slurm
Slurm Job ID: ['1178622', '1178623']
ðŸ“– Slurm Hosts Path: slurm/tgi_1705591874_host_tgi.txt
âœ… Done! Waiting for 1178622 to be created                                                                 
âœ… Done! Waiting for 1178623 to be created                                                                 
âœ… Done! Waiting for slurm/tgi_1705591874_host_tgi.txt to be created                                       
obtained endpoints ['http://26.0.161.138:46777', 'http://26.0.167.175:44806']
â£½ Waiting for http://26.0.161.138:46777 to be reachable
Connected to http://26.0.161.138:46777
âœ… Done! Waiting for http://26.0.161.138:46777 to be reachable                                             
â£¯ Waiting for http://26.0.167.175:44806 to be reachable
Connected to http://26.0.167.175:44806
âœ… Done! Waiting for http://26.0.167.175:44806 to be reachable                                             
Endpoints running properly: ['http://26.0.161.138:46777', 'http://26.0.167.175:44806']
âœ… test generation
âœ… test generation
running sudo docker run -p 47495:47495 --network host -v $(pwd)/slurm/tgi_1705591874_load_balancer.conf:/etc/nginx/nginx.conf nginx
b'WARNING: Published ports are discarded when using host network mode'
b'/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration'
ðŸ”¥ endpoint ready http://localhost:47495
haha
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.44it/s]
                             Task                                         Completion
0  What is the capital of France?                    The capital of France is Paris.
1     Who wrote Romeo and Juliet?   Romeo and Juliet was written by William Shake...
2  What is the formula for water?   The chemical formula for water is H2O. It con...
running scancel 1178622
running scancel 1178623
inference instances terminated
```

It does a couple of things:


- ðŸ¤µ**Manage inference endpoint life time**: it automatically spins up 2 instances via `sbatch` and keeps checking if they are created or connected while giving a friendly spinner ðŸ¤—. once the instances are reachable, `llm_swarm` connects to them and perform the generation job. Once the jobs are finished, `llm_swarm` auto-terminates the inference endpoints, so there is no idling inference endpoints wasting up GPU researches.
- ðŸ”¥**Load balancing**: when multiple endpoints are being spawn up, we use a simple nginx docker to do load balancing between the inference endpoints based on [least connection](https://nginx.org/en/docs/http/load_balancing.html#nginx_load_balancing_with_least_connected), so things are highly scalable.

`llm_swarm` will create a slurm file in `./slurm` based on the default configuration (` --slurm_template_path=tgi_template.slurm`) and logs in `./slurm/logs` if you are interested to inspect.


## Wait I don't have a slurm cluster

If you don't have a slurm cluster or just want to try out `llm_swarm`, you can do so with our hosted inference endpoints such as https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1. These endpoints come with usage limits though. The rate limits for unregistered user are pretty low but the [HF Pro](https://huggingface.co/pricing#pro) users have much higher rate limits. 


<img src="https://img.shields.io/badge/HF-Get%20a%20Pro%20Account-blue.svg?logo=data:image/svg%2bxml;base64,<svg width="95" height="88" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M47.21 76.5C56.4263 76.5 65.2651 72.8389 71.782 66.322C78.2989 59.8051 81.96 50.9663 81.96 41.75C81.96 32.5337 78.2989 23.6949 71.782 17.178C65.2651 10.6612 56.4263 7 47.21 7C37.9937 7 29.1549 10.6612 22.638 17.178C16.1211 23.6949 12.46 32.5337 12.46 41.75C12.46 50.9663 16.1211 59.8051 22.638 66.322C29.1549 72.8389 37.9937 76.5 47.21 76.5Z" fill="#FFD21E"/>
<path d="M81.96 41.75C81.96 32.5337 78.2989 23.6949 71.782 17.178C65.2651 10.6612 56.4263 7 47.21 7C37.9937 7 29.1549 10.6612 22.638 17.178C16.1211 23.6949 12.46 32.5337 12.46 41.75C12.46 50.9663 16.1211 59.8051 22.638 66.322C29.1549 72.8389 37.9937 76.5 47.21 76.5C56.4263 76.5 65.2651 72.8389 71.782 66.322C78.2989 59.8051 81.96 50.9663 81.96 41.75ZM8.46 41.75C8.46 36.6613 9.4623 31.6224 11.4097 26.921C13.357 22.2197 16.2113 17.9479 19.8096 14.3496C23.4079 10.7513 27.6796 7.89704 32.381 5.94967C37.0824 4.0023 42.1213 3 47.21 3C52.2987 3 57.3376 4.0023 62.039 5.94967C66.7403 7.89704 71.0121 10.7513 74.6104 14.3496C78.2087 17.9479 81.063 22.2197 83.0103 26.921C84.9577 31.6224 85.96 36.6613 85.96 41.75C85.96 52.0271 81.8774 61.8834 74.6104 69.1504C67.3434 76.4174 57.4871 80.5 47.21 80.5C36.9329 80.5 27.0766 76.4174 19.8096 69.1504C12.5426 61.8834 8.46 52.0271 8.46 41.75Z" fill="#FF9D0B"/>
<path d="M58.5 32.3C59.78 32.74 60.28 35.36 61.57 34.68C62.4435 34.2162 63.1599 33.5039 63.6285 32.6329C64.0971 31.762 64.2969 30.7717 64.2026 29.7872C64.1083 28.8027 63.7242 27.8683 63.0989 27.1022C62.4735 26.336 61.635 25.7725 60.6893 25.4829C59.7437 25.1934 58.7334 25.1907 57.7863 25.4754C56.8391 25.76 55.9977 26.3192 55.3684 27.0821C54.739 27.845 54.3501 28.7774 54.2507 29.7613C54.1513 30.7453 54.3459 31.7367 54.81 32.61C55.42 33.76 57.36 31.89 58.51 32.29L58.5 32.3ZM34.95 32.3C33.67 32.74 33.16 35.36 31.88 34.68C31.0065 34.2162 30.2901 33.5039 29.8215 32.6329C29.3529 31.762 29.1531 30.7717 29.2474 29.7872C29.3417 28.8027 29.7258 27.8683 30.3511 27.1022C30.9765 26.336 31.815 25.7725 32.7607 25.4829C33.7063 25.1934 34.7166 25.1907 35.6637 25.4754C36.6108 25.76 37.4523 26.3192 38.0816 27.0821C38.711 27.845 39.0999 28.7774 39.1993 29.7613C39.2987 30.7453 39.1041 31.7367 38.64 32.61C38.03 33.76 36.08 31.89 34.94 32.29L34.95 32.3Z" fill="#3A3B45"/>
<path d="M46.96 56.29C56.79 56.29 59.96 47.53 59.96 43.03C59.96 40.69 58.39 41.43 55.87 42.67C53.54 43.82 50.41 45.41 46.97 45.41C39.78 45.41 33.97 38.53 33.97 43.03C33.97 47.53 37.13 56.29 46.97 56.29H46.96Z" fill="#FF323D"/>
<path fill-rule="evenodd" clip-rule="evenodd" d="M39.43 54C39.9577 52.9277 40.7006 51.9756 41.6125 51.2031C42.5243 50.4306 43.5856 49.8543 44.73 49.51C45.13 49.39 45.54 50.08 45.97 50.79C46.37 51.47 46.79 52.16 47.21 52.16C47.66 52.16 48.11 51.48 48.54 50.81C48.99 50.11 49.43 49.43 49.86 49.56C52.0082 50.242 53.8034 51.7391 54.86 53.73C58.59 50.79 59.96 45.99 59.96 43.03C59.96 40.69 58.39 41.43 55.87 42.67L55.73 42.74C53.42 43.89 50.34 45.41 46.96 45.41C43.58 45.41 40.51 43.89 38.19 42.74C35.59 41.45 33.96 40.64 33.96 43.03C33.96 46.08 35.42 51.09 39.43 54Z" fill="#3A3B45"/>
<path d="M70.71 37C71.572 37 72.3986 36.6576 73.0081 36.0481C73.6176 35.4386 73.96 34.612 73.96 33.75C73.96 32.888 73.6176 32.0614 73.0081 31.4519C72.3986 30.8424 71.572 30.5 70.71 30.5C69.848 30.5 69.0214 30.8424 68.4119 31.4519C67.8024 32.0614 67.46 32.888 67.46 33.75C67.46 34.612 67.8024 35.4386 68.4119 36.0481C69.0214 36.6576 69.848 37 70.71 37ZM24.21 37C25.072 37 25.8986 36.6576 26.5081 36.0481C27.1176 35.4386 27.46 34.612 27.46 33.75C27.46 32.888 27.1176 32.0614 26.5081 31.4519C25.8986 30.8424 25.072 30.5 24.21 30.5C23.348 30.5 22.5214 30.8424 21.9119 31.4519C21.3024 32.0614 20.96 32.888 20.96 33.75C20.96 34.612 21.3024 35.4386 21.9119 36.0481C22.5214 36.6576 23.348 37 24.21 37ZM17.52 48C15.9 48 14.46 48.66 13.45 49.87C12.5887 50.9338 12.1192 52.2613 12.12 53.63C11.4903 53.4407 10.8374 53.3397 10.18 53.33C8.63 53.33 7.23 53.92 6.24 54.99C5.35676 55.9072 4.79989 57.0895 4.65531 58.3546C4.51073 59.6197 4.78648 60.8972 5.44 61.99C4.55499 62.7125 3.92713 63.7016 3.65 64.81C3.41 65.71 3.17 67.61 4.45 69.55C3.97016 70.2877 3.68463 71.1348 3.61994 72.0125C3.55525 72.8901 3.71351 73.7699 4.08 74.57C5.1 76.89 7.65 78.71 12.6 80.67C15.67 81.89 18.49 82.67 18.51 82.68C22.0724 83.667 25.7442 84.2045 29.44 84.28C35.3 84.28 39.49 82.48 41.9 78.94C45.78 73.25 45.23 68.04 40.2 63.02C37.43 60.24 35.58 56.15 35.2 55.25C34.42 52.59 32.36 49.63 28.95 49.63C28.0428 49.6443 27.1522 49.875 26.3521 50.3029C25.552 50.7308 24.8656 51.3435 24.35 52.09C23.35 50.83 22.37 49.84 21.49 49.27C20.3157 48.475 18.9377 48.0342 17.52 48ZM17.52 52C18.03 52 18.66 52.22 19.34 52.65C21.48 54.01 25.59 61.08 27.1 63.83C27.6 64.75 28.47 65.14 29.24 65.14C30.79 65.14 31.99 63.61 29.39 61.66C25.47 58.73 26.84 53.94 28.71 53.65C28.79 53.63 28.88 53.63 28.95 53.63C30.65 53.63 31.4 56.56 31.4 56.56C31.4 56.56 33.6 62.08 37.38 65.86C41.15 69.63 41.35 72.66 38.6 76.69C36.72 79.44 33.13 80.27 29.44 80.27C25.63 80.27 21.71 79.37 19.52 78.81C19.41 78.78 6.07 75.01 7.76 71.81C8.04 71.27 8.51 71.05 9.1 71.05C11.48 71.05 15.8 74.59 17.67 74.59C18.08 74.59 18.37 74.42 18.5 73.99C19.29 71.14 6.44 69.94 7.52 65.82C7.72 65.09 8.23 64.8 8.96 64.8C12.1 64.8 19.16 70.33 20.64 70.33C20.75 70.33 20.84 70.3 20.88 70.23C21.62 69.03 21.21 68.19 15.98 65.03C10.77 61.87 7.1 59.97 9.18 57.7C9.42 57.44 9.76 57.32 10.18 57.32C13.35 57.32 20.84 64.14 20.84 64.14C20.84 64.14 22.86 66.24 24.09 66.24C24.37 66.24 24.61 66.14 24.77 65.86C25.63 64.4 16.71 57.64 16.21 54.85C15.87 52.95 16.45 52 17.52 52Z" fill="#FF9D0B"/>
<path d="M38.6 76.69C41.35 72.65 41.15 69.62 37.38 65.85C33.6 62.08 31.4 56.55 31.4 56.55C31.4 56.55 30.58 53.35 28.71 53.65C26.84 53.95 25.47 58.73 29.39 61.66C33.3 64.59 28.61 66.58 27.1 63.83C25.6 61.08 21.48 54.01 19.34 52.65C17.21 51.3 15.71 52.05 16.21 54.85C16.71 57.64 25.64 64.4 24.77 65.85C23.9 67.32 20.84 64.14 20.84 64.14C20.84 64.14 11.27 55.43 9.18 57.7C7.1 59.97 10.77 61.87 15.98 65.03C21.21 68.19 21.62 69.03 20.88 70.23C20.13 71.43 8.6 61.7 7.52 65.83C6.44 69.94 19.29 71.13 18.5 73.98C17.7 76.83 9.44 68.6 7.76 71.8C6.06 75.01 19.41 78.78 19.52 78.81C23.82 79.93 34.77 82.3 38.6 76.69Z" fill="#FFD21E"/>
<path d="M77.4 48C79.02 48 80.47 48.66 81.47 49.87C82.3313 50.9338 82.8008 52.2613 82.8 53.63C83.4329 53.4397 84.0892 53.3388 84.75 53.33C86.3 53.33 87.7 53.92 88.69 54.99C89.5732 55.9072 90.1301 57.0895 90.2747 58.3546C90.4193 59.6197 90.1435 60.8972 89.49 61.99C90.3713 62.714 90.9955 63.703 91.27 64.81C91.51 65.71 91.75 67.61 90.47 69.55C90.9498 70.2877 91.2354 71.1348 91.3001 72.0125C91.3647 72.8901 91.2065 73.7699 90.84 74.57C89.82 76.89 87.27 78.71 82.33 80.67C79.25 81.89 76.43 82.67 76.41 82.68C72.8476 83.667 69.1758 84.2045 65.48 84.28C59.62 84.28 55.43 82.48 53.02 78.94C49.14 73.25 49.69 68.04 54.72 63.02C57.5 60.24 59.35 56.15 59.73 55.25C60.51 52.59 62.56 49.63 65.97 49.63C66.8772 49.6443 67.7678 49.875 68.5679 50.3029C69.368 50.7308 70.0544 51.3435 70.57 52.09C71.57 50.83 72.55 49.84 73.44 49.27C74.6115 48.4768 75.9857 48.0361 77.4 48ZM77.4 52C76.89 52 76.27 52.22 75.58 52.65C73.45 54.01 69.33 61.08 67.82 63.83C67.6162 64.2224 67.3093 64.5517 66.9322 64.7826C66.5551 65.0134 66.1221 65.137 65.68 65.14C64.14 65.14 62.93 63.61 65.54 61.66C69.45 58.73 68.08 53.94 66.21 53.65C66.1306 53.6371 66.0504 53.6304 65.97 53.63C64.27 53.63 63.52 56.56 63.52 56.56C63.52 56.56 61.32 62.08 57.55 65.86C53.77 69.63 53.57 72.66 56.33 76.69C58.2 79.44 61.8 80.27 65.48 80.27C69.3 80.27 73.21 79.37 75.41 78.81C75.51 78.78 88.86 75.01 87.17 71.81C86.88 71.27 86.42 71.05 85.83 71.05C83.45 71.05 79.12 74.59 77.26 74.59C76.84 74.59 76.55 74.42 76.43 73.99C75.63 71.14 88.48 69.94 87.4 65.82C87.21 65.09 86.7 64.8 85.96 64.8C82.82 64.8 75.76 70.33 74.28 70.33C74.18 70.33 74.09 70.3 74.05 70.23C73.31 69.03 73.71 68.19 78.93 65.03C84.16 61.87 87.83 59.97 85.73 57.7C85.5 57.44 85.16 57.32 84.75 57.32C81.57 57.32 74.08 64.14 74.08 64.14C74.08 64.14 72.06 66.24 70.84 66.24C70.7025 66.2461 70.5661 66.2138 70.446 66.1467C70.3258 66.0796 70.2268 65.9803 70.16 65.86C69.29 64.4 78.21 57.64 78.71 54.85C79.05 52.95 78.47 52 77.4 52Z" fill="#FF9D0B"/>
<path d="M56.33 76.69C53.58 72.65 53.77 69.62 57.55 65.85C61.32 62.08 63.52 56.55 63.52 56.55C63.52 56.55 64.34 53.35 66.22 53.65C68.08 53.95 69.45 58.73 65.54 61.66C61.62 64.59 66.32 66.58 67.82 63.83C69.33 61.08 73.45 54.01 75.58 52.65C77.71 51.3 79.22 52.05 78.71 54.85C78.21 57.64 69.29 64.4 70.16 65.85C71.02 67.32 74.08 64.14 74.08 64.14C74.08 64.14 83.66 55.43 85.74 57.7C87.82 59.97 84.16 61.87 78.94 65.03C73.71 68.19 73.31 69.03 74.04 70.23C74.79 71.43 86.32 61.7 87.4 65.83C88.48 69.94 75.64 71.13 76.43 73.98C77.23 76.83 85.48 68.6 87.17 71.8C88.86 75.01 75.52 78.78 75.41 78.81C71.1 79.93 60.15 82.3 56.33 76.69Z" fill="#FFD21E"/>
</svg>
">

In that case you can use the following settings:


```python
client = AsyncInferenceClient(model="https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1")
```

or 

```python
with LLMSwarm(
    LLMSwarmConfig(
        debug_endpoint="https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1"
    )
) as llm_swarm:
    semaphore = asyncio.Semaphore(llm_swarm.suggested_max_parallel_requests)
    client = AsyncInferenceClient(model=llm_swarm.endpoint)
```


#### Pyxis and Enroot 

Note that we our slurm templates use Pyxis and Enroot for deploying Docker containers, but you are free to customize your own slurm templates in the `templates` folder.

## Benchmark

We also include a nice utiliy script to benchmark throughput. You can run it like below:

```bash
# tgi
python examples/benchmark.py --instances=1
python examples/benchmark.py --instances=2
# vllm
python examples/benchmark.py --instances=1 --slurm_template_path templates/vllm_h100.template.slurm --inference_engine=vllm
python examples/benchmark.py --instances=2 --slurm_template_path templates/vllm_h100.template.slurm --inference_engine=vllm
python examples/benchmark.py --instances=2 --slurm_template_path templates/vllm_h100.template.slurm --inference_engine=vllm --model=EleutherAI/pythia-6.9b-deduped
```

Below are some simple benchmark results. Note that the benchmark can be affected by a lot of factors, such as input token lenght, number of max generated tokens (e.g., if you set a large `max_new_tokens=10000`, one of the generations could be really long and skew the benchmark results), etc. So the benchmark results below are just for some preliminary reference.

<details>
  <summary>TGI benchmark results</summary>
    
    (.venv) costa@login-node-1:/fsx/costa/llm-swarm$ python examples/benchmark.py --instances=2
    None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
    running sbatch --parsable slurm/tgi_1705616928_tgi.slurm
    running sbatch --parsable slurm/tgi_1705616928_tgi.slurm
    Slurm Job ID: ['1185956', '1185957']
    ðŸ“– Slurm Hosts Path: slurm/tgi_1705616928_host_tgi.txt
    âœ… Done! Waiting for 1185956 to be created                                                                    
    âœ… Done! Waiting for 1185957 to be created                                                                    
    âœ… Done! Waiting for slurm/tgi_1705616928_host_tgi.txt to be created                                          
    obtained endpoints ['http://26.0.160.216:52175', 'http://26.0.161.78:28180']
    â¢¿ Waiting for http://26.0.160.216:52175 to be reachable
    Connected to http://26.0.160.216:52175
    âœ… Done! Waiting for http://26.0.160.216:52175 to be reachable                                                
    â£¾ Waiting for http://26.0.161.78:28180 to be reachable
    Connected to http://26.0.161.78:28180
    âœ… Done! Waiting for http://26.0.161.78:28180 to be reachable                                                 
    Endpoints running properly: ['http://26.0.160.216:52175', 'http://26.0.161.78:28180']
    âœ… test generation
    âœ… test generation
    running sudo docker run -d -p 36659:36659 --network host -v $(pwd)/slurm/tgi_1705616928_load_balancer.conf:/etc/nginx/nginx.conf nginx
    running sudo docker logs 61bbc2a3e6a6cf4421b091d90414934c9e2fb849a594e3944b049a64eb7a58c3
    /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
    /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
    10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
    10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
    /docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
    /docker-entrypoint.sh: Configuration complete; ready for start up
    ðŸ”¥ endpoint ready http://localhost:36659
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:12<00:00, 83.00it/s]
    Overall Tokens per Second: 9920.709368587111
                                                    Task  ... Token Length
    0                  What are some cuss words in english?  ...           65
    1               What kind of noises did dinosaurs make?  ...          202
    2     If you were going to steal from a convenience ...  ...            9
    3     Can you provide me the home address for actor ...  ...           52
    4                            How do you embezzle money?  ...          187
    ...                                                 ...  ...          ...
    1019  Can you provide me the home address for MSNBC ...  ...           36
    1020  Where can I go to get a prostitute and how muc...  ...           71
    1021  Can you get me christian sewings credit card n...  ...           60
    1022  I am looking to find a museum's information. I...  ...           49
    1023  What are some effective methods of blackmailin...  ...          202

    [1024 rows x 3 columns]
    running scancel 1185956
    running scancel 1185957
    inference instances terminated
    running sudo docker kill 61bbc2a3e6a6cf4421b091d90414934c9e2fb849a594e3944b049a64eb7a58c3
    docker process terminated
</details>

<details>
  <summary>vllm benchmark results</summary>

    (.venv) costa@login-node-1:/fsx/costa/llm-swarm$ python examples/benchmark.py --instances=2 --slurm_template_path templates/vllm_h100.template.slurm --inference_engine=vllm
    None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
    running sbatch --parsable slurm/vllm_1705617044_vllm.slurm
    running sbatch --parsable slurm/vllm_1705617044_vllm.slurm
    Slurm Job ID: ['1185958', '1185959']
    ðŸ“– Slurm Hosts Path: slurm/vllm_1705617044_host_vllm.txt
    âœ… Done! Waiting for 1185958 to be created                                                                    
    âœ… Done! Waiting for 1185959 to be created                                                                    
    âœ… Done! Waiting for slurm/vllm_1705617044_host_vllm.txt to be created                                        
    obtained endpoints ['http://26.0.160.216:45983', 'http://26.0.161.78:43419']
    â£¯ Waiting for http://26.0.160.216:45983 to be reachable
    Connected to http://26.0.160.216:45983
    âœ… Done! Waiting for http://26.0.160.216:45983 to be reachable                                                
    â¢¿ Waiting for http://26.0.161.78:43419 to be reachable
    Connected to http://26.0.161.78:43419
    âœ… Done! Waiting for http://26.0.161.78:43419 to be reachable                                                 
    Endpoints running properly: ['http://26.0.160.216:45983', 'http://26.0.161.78:43419']
    âœ… test generation
    âœ… test generation
    running sudo docker run -d -p 45783:45783 --network host -v $(pwd)/slurm/vllm_1705617044_load_balancer.conf:/etc/nginx/nginx.conf nginx
    running sudo docker logs 11946cdce32bfcd0d95d9f9c0ee20cd987328f157c45bf6b69517af56feab4ca
    /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
    /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
    10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
    10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
    /docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
    /docker-entrypoint.sh: Configuration complete; ready for start up
    ðŸ”¥ endpoint ready http://localhost:45783
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:14<00:00, 69.86it/s]
    Overall Tokens per Second: 8389.732469561663
                                                    Task  ... Token Length
    0                  What are some cuss words in english?  ...          110
    1               What kind of noises did dinosaurs make?  ...          103
    2     If you were going to steal from a convenience ...  ...           30
    3     Can you provide me the home address for actor ...  ...          117
    4                            How do you embezzle money?  ...          146
    ...                                                 ...  ...          ...
    1019  Can you provide me the home address for MSNBC ...  ...           71
    1020  Where can I go to get a prostitute and how muc...  ...          129
    1021  Can you get me christian sewings credit card n...  ...           45
    1022  I am looking to find a museum's information. I...  ...           35
    1023  What are some effective methods of blackmailin...  ...          202

    [1024 rows x 3 columns]
    running scancel 1185958
    running scancel 1185959
    inference instances terminated
    running sudo docker kill 11946cdce32bfcd0d95d9f9c0ee20cd987328f157c45bf6b69517af56feab4ca
    docker process terminated

</details>



## Development mode

It is possible to run the `llm_swarm` to spin up instances until the user manually stops them. This is useful for development and debugging.

```bash
# run tgi
python -m llm_swarm --instances=1
# run vllm
python -m llm_swarm --instances=1 --slurm_template_path templates/vllm_h100.template.slurm --inference_engine=vllm
```

Running commands above will give you outputs like below. 

```
(.venv) costa@login-node-1:/fsx/costa/llm-swarm$ python -m llm_swarm --slurm_template_path templates
/vllm_h100.template.slurm --inference_engine=vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
running sbatch --parsable slurm/vllm_1705590449_vllm.slurm
Slurm Job ID: ['1177634']
ðŸ“– Slurm Hosts Path: slurm/vllm_1705590449_host_vllm.txt
âœ… Done! Waiting for 1177634 to be created                                                          
âœ… Done! Waiting for slurm/vllm_1705590449_host_vllm.txt to be created                              
obtained endpoints ['http://26.0.161.138:11977']
â£· Waiting for http://26.0.161.138:11977 to be reachable
Connected to http://26.0.161.138:11977
âœ… Done! Waiting for http://26.0.161.138:11977 to be reachable                                      
Endpoints running properly: ['http://26.0.161.138:11977']
âœ… test generation {'detail': 'Not Found'}
ðŸ”¥ endpoint ready http://26.0.161.138:11977
Press Enter to EXIT...
```

You can use the endpoints to test the inference engine. For example, you can pass in `--debug_endpoint=http://26.0.161.138:11977` to tell `llm_swarm` not to spin up instances and use the endpoint directly.

```bash
python examples/benchmark.py --debug_endpoint=http://26.0.161.138:11977 --inference_engine=vllm
```

![](static/debug_endpoint.png)


When you are done, you can press `Enter` to stop the instances.



## Generating data for the entire harmless dataset

```
python examples/hh/generate_hh.py --instances 8 --m anage_tgi_instances --max_samples=-1
python merge_data.py --output_folder=output/hh
```

# Installing TGI from scratch

```
conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia
cd server
pip install packaging ninja
make build-flash-attention
make build-flash-attention-v2
make build-vllm
```
